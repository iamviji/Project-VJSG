{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasOneHot_10_20_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamviji/Project-VJSG/blob/master/PostMidTerm/Experiment/KerasOneHot_10_20_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5dLvr1Rr1K8"
      },
      "source": [
        "import numpy\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, GaussianNoise\n",
        "from tensorflow.keras import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior ()\n",
        "\n",
        "input_message_length = 10\n",
        "encoder_output_length = 20\n",
        "channel_size = 10\n",
        "NUM_OF_INPUT_MESSAGE = 1000\n",
        "SNR_STEP_SIZE = .5\n",
        "\n",
        "SNR_BEGIN = 0\n",
        "SNR_END = 10\n",
        "\n",
        "bler_per_iter_uncoded_commpy_psk_2= [0.521, 0.473, 0.436, 0.37,  0.304, 0.259, 0.187, 0.138, 0.098, 0.098, 0.052, 0.028, 0.012, 0.011, 0.009, 0.002, 0.0,  0.001, 0.,    0.0]\n",
        "bler_per_iter_uncoded_itpp_psk_2= [0.518, 0.478, 0.415, 0.355, 0.305, 0.227, 0.177, 0.149, 0.11,  0.075, 0.055, 0.023, 0.014, 0.014, 0.015, 0.001, 0.003, 0.001, 0.,    0. ]\n",
        "bler_per_iter_uncoded_commpy_psk_4 = [0.815, 0.793, 0.75,  0.714, 0.64,  0.639, 0.526, 0.49,  0.433, 0.371, 0.335, 0.236, 0.204, 0.154, 0.129, 0.08,  0.063, 0.046, 0.023, 0.018]\n",
        "bler_per_iter_uncoded_itpp_psk_4 = [0.814, 0.767, 0.729, 0.702, 0.66,  0.616, 0.563, 0.511, 0.442, 0.4,   0.294, 0.277, 0.228, 0.17,  0.114, 0.087, 0.05,  0.037, 0.022, 0.017]\n",
        "bler_per_iter_ldpc_itpp_psk_4 = [0.584, 0.488, 0.404, 0.332, 0.218, 0.151, 0.097, 0.058, 0.041, 0.024, 0.007, 0.004, 0.002, 0.001, 0.001, 0.,    0.,    0.,    0.,    0.,   ]\n",
        "bler_per_iter_ham_itpp_psk_4= [0.51, 0.479, 0.419, 0.333, 0.313, 0.247, 0.212, 0.132, 0.114, 0.093, 0.042, 0.027, 0.024, 0.016, 0.006, 0.005, 0.003, 0.002, 0.,    0.  ]\n",
        "\n",
        "\n",
        "def Snr2Sigma(snr):\n",
        "  sigma = 10 ** (- snr / 20)\n",
        "  return sigma\n",
        "\n",
        "\n",
        "def timer_update(i,current,time_tot,tic_incr=500):\n",
        "    last = current\n",
        "    current = time.time()\n",
        "    t_diff = current-last\n",
        "    print('SNR: {:04.3f} - Iter: {} - Last {} iterations took {:03.2f}s'.format(snr,i+1,tic_incr,t_diff))\n",
        "    return time_tot + t_diff\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIqZA3RCrJgd",
        "outputId": "ffd5f23e-18d0-4cc9-ddf7-7d6bf78de92d"
      },
      "source": [
        "\n",
        "awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [2*channel_size])\n",
        "awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "\n",
        "decoder_input_x = tf.placeholder(\"float32\", [None, input_message_length], name=\"decoder_input_x\")\n",
        "\n",
        "snr_std = 7.0\n",
        "\n",
        "input_message_x = Input(shape=(2**input_message_length,))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "enc_layer1 = Dense(encoder_output_length, activation='tanh')(input_message_x)\n",
        "enc_layer2 = Dense(2*channel_size, activation='tanh')(enc_layer1)\n",
        "enc_layer3 =  enc_layer2 / tf.sqrt(tf.reduce_mean(tf.square(enc_layer2)))\n",
        "encoder = Model(input_message_x, enc_layer3)\n",
        "\n",
        "awgn_channel = GaussianNoise(Snr2Sigma(snr_std),input_shape=(2*channel_size,))\n",
        "\n",
        "# create a placeholder for an encoded (32-dimensional) input\n",
        "encoded_input = Input(shape=(2*channel_size,))\n",
        "dec_layer1 = Dense(encoder_output_length, activation='tanh')(encoded_input)\n",
        "dec_layer2 = Dense(2**input_message_length, activation=\"softmax\")(dec_layer1)\n",
        "# this model maps an encoded input to its decoder representation\n",
        "decoder = Model(encoded_input, dec_layer2)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "\n",
        "\n",
        "print(encoder.summary())\n",
        "print(decoder.summary())\n",
        "print(autoencoder.summary())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_75\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, 1024)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 20)           20500       input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 20)           420         dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Square_4 (TensorFlo multiple             0           dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mean_4 (TensorFlowO multiple             0           tf_op_layer_Square_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sqrt_4 (TensorFlowO multiple             0           tf_op_layer_Mean_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_truediv_4 (TensorFl multiple             0           dense_17[0][0]                   \n",
            "                                                                 tf_op_layer_Sqrt_4[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 20,920\n",
            "Trainable params: 20,920\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"functional_77\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 20)]              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1024)              21504     \n",
            "=================================================================\n",
            "Total params: 21,924\n",
            "Trainable params: 21,924\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"functional_79\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 1024)]            0         \n",
            "_________________________________________________________________\n",
            "functional_75 (Functional)   (None, 20)                20920     \n",
            "_________________________________________________________________\n",
            "gaussian_noise_29 (GaussianN (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "functional_77 (Functional)   (None, 1024)              21924     \n",
            "=================================================================\n",
            "Total params: 42,844\n",
            "Trainable params: 42,844\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IidQMKlts65l",
        "outputId": "369e12dc-3c27-4a30-c552-8e2760d2f0f8"
      },
      "source": [
        "training_input_message = numpy.random.randint(2**input_message_length, size=(1,NUM_OF_INPUT_MESSAGE*10))\n",
        "training_input_message_one_hot = numpy.zeros((training_input_message.size, 2**input_message_length))\n",
        "training_input_message_one_hot[numpy.arange(training_input_message.size),training_input_message] = 1\n",
        "print(training_input_message_one_hot)\n",
        "print (training_input_message_one_hot.shape)\n",
        "print (training_input_message.shape)\n",
        "\n",
        "test_input_message = numpy.random.randint(2**input_message_length, size=(1,NUM_OF_INPUT_MESSAGE*2))\n",
        "test_input_message_one_hot = numpy.zeros((test_input_message.size, 2**input_message_length))\n",
        "test_input_message_one_hot[numpy.arange(test_input_message.size),test_input_message] = 1\n",
        "print(test_input_message_one_hot)\n",
        "print (test_input_message_one_hot.shape)\n",
        "print (test_input_message.shape)\n",
        "\n",
        "training_input_message_label = training_input_message.reshape(training_input_message.shape[1])\n",
        "print (training_input_message)\n",
        "print (training_input_message_label)\n",
        "test_input_message_label = test_input_message.reshape(test_input_message.shape[1])\n",
        "print (test_input_message)\n",
        "print (test_input_message_label)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(10000, 1024)\n",
            "(1, 10000)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(2000, 1024)\n",
            "(1, 2000)\n",
            "[[577 121 803 ... 525 731 153]]\n",
            "[577 121 803 ... 525 731 153]\n",
            "[[645 793 399 ... 699 231 718]]\n",
            "[645 793 399 ... 699 231 718]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbiBvRFNtUly",
        "outputId": "64adf20e-6052-45cf-d44d-aac8d7f046a8"
      },
      "source": [
        "import keras\n",
        "\n",
        "#def custom_losff_fucntion (act, pred):\n",
        "#  return (tf.reduce_mean(-1*(act * tf.log(pred) + (1-act)*tf.log(1-pred))))\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "#autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "#autoencoder.compile(optimizer=opt, loss=custom_losff_fucntion)\n",
        "#loss='mean_squared_error'\n",
        "#for snr in (numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)):\n",
        "for snr in (numpy.arange (0, 10, SNR_STEP_SIZE)):\n",
        "  sigma = 1.0*Snr2Sigma (snr)\n",
        "  snr_std = sigma\n",
        "  print (\"Training for SNR=\", snr, \" sigma=\", sigma) \n",
        "  awgn_channel = GaussianNoise(sigma,input_shape=(channel_size,))\n",
        "  autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.003)\n",
        "  autoencoder.compile(optimizer=opt, loss=loss_fn)\n",
        "  autoencoder.fit(training_input_message_one_hot, training_input_message_label,\n",
        "                epochs=50,\n",
        "                #epochs=20,\n",
        "                batch_size=500,\n",
        "                shuffle=False,\n",
        "                validation_data=(test_input_message_one_hot, test_input_message_label))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for SNR= 0.0  sigma= 1.0\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 2.8035 - val_loss: 0.0011\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 2.5172 - val_loss: 0.0012\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 2.3900 - val_loss: 0.0013\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 2.3488 - val_loss: 0.0013\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 2.2237 - val_loss: 0.0015\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 2.3000 - val_loss: 0.0013\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 2.2083 - val_loss: 0.0014\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 2.2134 - val_loss: 0.0014\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 2.1744 - val_loss: 0.0014\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 2.1902 - val_loss: 0.0014\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 2.1640 - val_loss: 0.0015\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 2.1477 - val_loss: 0.0015\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 2.0902 - val_loss: 0.0015\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 2.0372 - val_loss: 0.0016\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 2.0419 - val_loss: 0.0016\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.9728 - val_loss: 0.0017\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 2.0016 - val_loss: 0.0017\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 1.9773 - val_loss: 0.0017\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.9602 - val_loss: 0.0018\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.9097 - val_loss: 0.0018\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 1.9218 - val_loss: 0.0019\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 1.8478 - val_loss: 0.0020\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.8692 - val_loss: 0.0021\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.8585 - val_loss: 0.0021\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.7916 - val_loss: 0.0022\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.8004 - val_loss: 0.0023\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.7394 - val_loss: 0.0024\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.7945 - val_loss: 0.0026\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 1.7365 - val_loss: 0.0027\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 1.7177 - val_loss: 0.0030\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.7519 - val_loss: 0.0032\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.6645 - val_loss: 0.0034\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 1.6428 - val_loss: 0.0038\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.6338 - val_loss: 0.0041\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 1.6028 - val_loss: 0.0045\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.6385 - val_loss: 0.0049\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.6208 - val_loss: 0.0054\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.6059 - val_loss: 0.0060\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 1.5737 - val_loss: 0.0067\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 1.5516 - val_loss: 0.0074\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.5152 - val_loss: 0.0082\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.5355 - val_loss: 0.0091\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.4835 - val_loss: 0.0100\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.5280 - val_loss: 0.0112\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.4726 - val_loss: 0.0122\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.4912 - val_loss: 0.0132\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.4777 - val_loss: 0.0141\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 1.4500 - val_loss: 0.0149\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 1.4849 - val_loss: 0.0158\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.4752 - val_loss: 0.0167\n",
            "Training for SNR= 0.5  sigma= 0.9440608762859234\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: 1.2439 - val_loss: 0.0191\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 1.2773 - val_loss: 0.0187\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 1.2809 - val_loss: 0.0177\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 1.2516 - val_loss: 0.0170\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 1.2594 - val_loss: 0.0166\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 1.2785 - val_loss: 0.0162\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2558 - val_loss: 0.0159\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2633 - val_loss: 0.0159\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.2721 - val_loss: 0.0159\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2364 - val_loss: 0.0157\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.2417 - val_loss: 0.0155\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.2483 - val_loss: 0.0155\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.2386 - val_loss: 0.0153\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 1.2302 - val_loss: 0.0150\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.2408 - val_loss: 0.0147\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 1.2361 - val_loss: 0.0146\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2111 - val_loss: 0.0147\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2250 - val_loss: 0.0145\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 1.2417 - val_loss: 0.0144\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 1.2409 - val_loss: 0.0145\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2379 - val_loss: 0.0142\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.2343 - val_loss: 0.0140\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.2325 - val_loss: 0.0139\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 1.2149 - val_loss: 0.0137\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2363 - val_loss: 0.0135\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 1.2485 - val_loss: 0.0136\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2163 - val_loss: 0.0136\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 1.2268 - val_loss: 0.0134\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.2026 - val_loss: 0.0133\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2082 - val_loss: 0.0133\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 1.2416 - val_loss: 0.0135\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2099 - val_loss: 0.0133\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 1.2762 - val_loss: 0.0134\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.2215 - val_loss: 0.0135\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 1.2245 - val_loss: 0.0134\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2239 - val_loss: 0.0133\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 1.2002 - val_loss: 0.0132\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2153 - val_loss: 0.0131\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 1.2031 - val_loss: 0.0131\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 1.1995 - val_loss: 0.0129\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.1874 - val_loss: 0.0126\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.1792 - val_loss: 0.0125\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 1.2035 - val_loss: 0.0125\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2089 - val_loss: 0.0126\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 1.1941 - val_loss: 0.0125\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.2077 - val_loss: 0.0123\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 1.1968 - val_loss: 0.0122\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 1.2148 - val_loss: 0.0121\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 1.2008 - val_loss: 0.0121\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 1.1991 - val_loss: 0.0119\n",
            "Training for SNR= 1.0  sigma= 0.8912509381337456\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 104us/sample - loss: 1.0088 - val_loss: 0.0121\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9988 - val_loss: 0.0113\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9883 - val_loss: 0.0109\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 1.0002 - val_loss: 0.0102\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9881 - val_loss: 0.0099\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.9906 - val_loss: 0.0096\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9767 - val_loss: 0.0094\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9817 - val_loss: 0.0091\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.9903 - val_loss: 0.0088\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9912 - val_loss: 0.0087\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.9559 - val_loss: 0.0086\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9729 - val_loss: 0.0084\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.9907 - val_loss: 0.0082\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.9704 - val_loss: 0.0081\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.9586 - val_loss: 0.0082\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.9714 - val_loss: 0.0080\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.9715 - val_loss: 0.0078\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.9734 - val_loss: 0.0078\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.9571 - val_loss: 0.0077\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.9793 - val_loss: 0.0076\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.9437 - val_loss: 0.0076\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9614 - val_loss: 0.0074\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9402 - val_loss: 0.0073\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.9605 - val_loss: 0.0073\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.9567 - val_loss: 0.0072\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9641 - val_loss: 0.0072\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9773 - val_loss: 0.0072\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9448 - val_loss: 0.0072\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9418 - val_loss: 0.0072\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.9659 - val_loss: 0.0072\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9529 - val_loss: 0.0070\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9482 - val_loss: 0.0071\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9487 - val_loss: 0.0070\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9490 - val_loss: 0.0070\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9439 - val_loss: 0.0070\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9583 - val_loss: 0.0070\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9138 - val_loss: 0.0070\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9471 - val_loss: 0.0069\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.9473 - val_loss: 0.0068\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9427 - val_loss: 0.0069\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.9493 - val_loss: 0.0068\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9790 - val_loss: 0.0068\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9380 - val_loss: 0.0068\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9256 - val_loss: 0.0067\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9515 - val_loss: 0.0067\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9576 - val_loss: 0.0067\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9645 - val_loss: 0.0067\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.9589 - val_loss: 0.0067\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.9298 - val_loss: 0.0066\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.9488 - val_loss: 0.0065\n",
            "Training for SNR= 1.5  sigma= 0.8413951416451951\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.7649 - val_loss: 0.0067\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.7772 - val_loss: 0.0064\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.7395 - val_loss: 0.0060\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.7238 - val_loss: 0.0058\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7453 - val_loss: 0.0055\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7435 - val_loss: 0.0054\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.7268 - val_loss: 0.0052\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.7239 - val_loss: 0.0051\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7404 - val_loss: 0.0050\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.7276 - val_loss: 0.0049\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.7494 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.7114 - val_loss: 0.0047\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.7428 - val_loss: 0.0046\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.7426 - val_loss: 0.0045\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7254 - val_loss: 0.0045\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7430 - val_loss: 0.0044\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7307 - val_loss: 0.0044\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7411 - val_loss: 0.0044\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.7280 - val_loss: 0.0044\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7298 - val_loss: 0.0044\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7346 - val_loss: 0.0043\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.7265 - val_loss: 0.0042\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.7234 - val_loss: 0.0042\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7197 - val_loss: 0.0041\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.7649 - val_loss: 0.0041\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7267 - val_loss: 0.0041\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7226 - val_loss: 0.0040\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.7365 - val_loss: 0.0041\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.7440 - val_loss: 0.0040\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.7344 - val_loss: 0.0041\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7311 - val_loss: 0.0041\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7169 - val_loss: 0.0041\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7251 - val_loss: 0.0041\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.7112 - val_loss: 0.0041\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.7503 - val_loss: 0.0040\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.7235 - val_loss: 0.0039\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7265 - val_loss: 0.0039\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7201 - val_loss: 0.0039\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.7363 - val_loss: 0.0039\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.7356 - val_loss: 0.0039\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.7181 - val_loss: 0.0039\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7217 - val_loss: 0.0039\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7363 - val_loss: 0.0039\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.7025 - val_loss: 0.0038\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.7122 - val_loss: 0.0037\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7050 - val_loss: 0.0037\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.7058 - val_loss: 0.0037\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.6971 - val_loss: 0.0037\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.7129 - val_loss: 0.0036\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.7140 - val_loss: 0.0036\n",
            "Training for SNR= 2.0  sigma= 0.7943282347242815\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 111us/sample - loss: 0.5528 - val_loss: 0.0037\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.5620 - val_loss: 0.0036\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5525 - val_loss: 0.0034\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5486 - val_loss: 0.0033\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.5397 - val_loss: 0.0032\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.5414 - val_loss: 0.0032\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.5476 - val_loss: 0.0030\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5384 - val_loss: 0.0030\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.5549 - val_loss: 0.0029\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5619 - val_loss: 0.0028\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 0s 46us/sample - loss: 0.5363 - val_loss: 0.0028\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5409 - val_loss: 0.0027\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5448 - val_loss: 0.0027\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.5367 - val_loss: 0.0027\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5345 - val_loss: 0.0026\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5423 - val_loss: 0.0026\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.5501 - val_loss: 0.0026\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.5432 - val_loss: 0.0026\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5290 - val_loss: 0.0025\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5563 - val_loss: 0.0025\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.5576 - val_loss: 0.0025\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5286 - val_loss: 0.0025\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.5453 - val_loss: 0.0025\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5306 - val_loss: 0.0025\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5222 - val_loss: 0.0025\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.5467 - val_loss: 0.0024\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5485 - val_loss: 0.0024\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5482 - val_loss: 0.0024\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5386 - val_loss: 0.0024\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.5450 - val_loss: 0.0023\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5406 - val_loss: 0.0024\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.5416 - val_loss: 0.0024\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.5243 - val_loss: 0.0023\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5307 - val_loss: 0.0023\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5259 - val_loss: 0.0023\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5463 - val_loss: 0.0023\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5298 - val_loss: 0.0023\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5325 - val_loss: 0.0023\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5441 - val_loss: 0.0023\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5309 - val_loss: 0.0023\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5406 - val_loss: 0.0023\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5248 - val_loss: 0.0023\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5413 - val_loss: 0.0022\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5337 - val_loss: 0.0022\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5134 - val_loss: 0.0022\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5288 - val_loss: 0.0022\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5251 - val_loss: 0.0022\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.5409 - val_loss: 0.0022\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.5245 - val_loss: 0.0022\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.5284 - val_loss: 0.0022\n",
            "Training for SNR= 2.5  sigma= 0.7498942093324559\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 107us/sample - loss: 0.3856 - val_loss: 0.0022\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.4007 - val_loss: 0.0021\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.4045 - val_loss: 0.0021\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.3774 - val_loss: 0.0020\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3992 - val_loss: 0.0020\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.4011 - val_loss: 0.0019\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3754 - val_loss: 0.0019\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3821 - val_loss: 0.0018\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.4030 - val_loss: 0.0018\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3863 - val_loss: 0.0017\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.4005 - val_loss: 0.0017\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.3928 - val_loss: 0.0017\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3797 - val_loss: 0.0017\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.3817 - val_loss: 0.0017\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.3812 - val_loss: 0.0016\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3992 - val_loss: 0.0016\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.3704 - val_loss: 0.0016\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.3953 - val_loss: 0.0016\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.3889 - val_loss: 0.0016\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3938 - val_loss: 0.0016\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3855 - val_loss: 0.0016\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3906 - val_loss: 0.0016\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3887 - val_loss: 0.0016\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3916 - val_loss: 0.0015\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.3807 - val_loss: 0.0015\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3900 - val_loss: 0.0015\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3893 - val_loss: 0.0015\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.3915 - val_loss: 0.0015\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3709 - val_loss: 0.0015\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.3785 - val_loss: 0.0014\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3972 - val_loss: 0.0014\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3827 - val_loss: 0.0014\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.3767 - val_loss: 0.0014\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3779 - val_loss: 0.0014\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.3823 - val_loss: 0.0014\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3894 - val_loss: 0.0014\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.3872 - val_loss: 0.0014\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.3891 - val_loss: 0.0014\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3706 - val_loss: 0.0014\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.3867 - val_loss: 0.0014\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3694 - val_loss: 0.0014\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3871 - val_loss: 0.0014\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.3896 - val_loss: 0.0014\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.3800 - val_loss: 0.0013\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.3715 - val_loss: 0.0013\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.3751 - val_loss: 0.0013\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3815 - val_loss: 0.0013\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3756 - val_loss: 0.0013\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.3775 - val_loss: 0.0013\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3662 - val_loss: 0.0013\n",
            "Training for SNR= 3.0  sigma= 0.7079457843841379\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 107us/sample - loss: 0.2716 - val_loss: 0.0013\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2793 - val_loss: 0.0013\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2836 - val_loss: 0.0013\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.2716 - val_loss: 0.0013\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2695 - val_loss: 0.0013\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.2624 - val_loss: 0.0012\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2824 - val_loss: 0.0012\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2690 - val_loss: 0.0012\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2642 - val_loss: 0.0011\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.2740 - val_loss: 0.0011\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.2708 - val_loss: 0.0011\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.2721 - val_loss: 0.0011\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.2643 - val_loss: 0.0011\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.2657 - val_loss: 0.0011\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2753 - val_loss: 0.0011\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.2637 - val_loss: 0.0011\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.2838 - val_loss: 0.0010\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.2719 - val_loss: 0.0010\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.2737 - val_loss: 0.0010\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.2730 - val_loss: 0.0010\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.2701 - val_loss: 0.0010\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.2707 - val_loss: 0.0010\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.2666 - val_loss: 0.0010\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.2674 - val_loss: 9.9175e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.2642 - val_loss: 9.9950e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.2776 - val_loss: 9.7601e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.2750 - val_loss: 9.6585e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.2758 - val_loss: 9.5724e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.2810 - val_loss: 9.5022e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2770 - val_loss: 9.6456e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2685 - val_loss: 9.5918e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.2593 - val_loss: 9.5349e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2681 - val_loss: 9.4625e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2719 - val_loss: 9.2429e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.2622 - val_loss: 9.2934e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2605 - val_loss: 9.1691e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.2664 - val_loss: 9.1900e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2584 - val_loss: 9.2781e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2624 - val_loss: 9.0212e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2636 - val_loss: 9.0043e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2639 - val_loss: 8.9636e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.2691 - val_loss: 8.8914e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2617 - val_loss: 8.7307e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2767 - val_loss: 8.8253e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2500 - val_loss: 8.8163e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2767 - val_loss: 8.9019e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2614 - val_loss: 8.8280e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.2748 - val_loss: 8.7871e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.2588 - val_loss: 8.7869e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.2535 - val_loss: 8.8640e-04\n",
            "Training for SNR= 3.5  sigma= 0.6683439175686147\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 110us/sample - loss: 0.1787 - val_loss: 9.1122e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1939 - val_loss: 8.8092e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.1863 - val_loss: 8.6176e-04\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1750 - val_loss: 8.4872e-04\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1822 - val_loss: 8.1961e-04\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1847 - val_loss: 8.0311e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1838 - val_loss: 7.9918e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1915 - val_loss: 7.9742e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1834 - val_loss: 7.8750e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1855 - val_loss: 7.8415e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.1835 - val_loss: 7.6065e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1821 - val_loss: 7.5624e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.1827 - val_loss: 7.2923e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.1825 - val_loss: 7.3582e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1767 - val_loss: 7.3525e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1702 - val_loss: 7.3246e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.1773 - val_loss: 7.1632e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.1839 - val_loss: 7.1034e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1797 - val_loss: 6.9669e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1766 - val_loss: 6.8875e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1814 - val_loss: 6.9273e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1828 - val_loss: 7.0158e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.1898 - val_loss: 6.9129e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1900 - val_loss: 6.8695e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1805 - val_loss: 6.9024e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1733 - val_loss: 6.7952e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.1814 - val_loss: 6.6625e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1879 - val_loss: 6.6683e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.1676 - val_loss: 6.6425e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1774 - val_loss: 6.5214e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1781 - val_loss: 6.4759e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1842 - val_loss: 6.3973e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1773 - val_loss: 6.4230e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1928 - val_loss: 6.4844e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1735 - val_loss: 6.4426e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.1888 - val_loss: 6.3225e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1790 - val_loss: 6.2850e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.1856 - val_loss: 6.3297e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1757 - val_loss: 6.3112e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.1741 - val_loss: 6.1728e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.1810 - val_loss: 6.2141e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1769 - val_loss: 6.2170e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1760 - val_loss: 6.0887e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1686 - val_loss: 6.0364e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1807 - val_loss: 6.0315e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.1779 - val_loss: 5.9674e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1653 - val_loss: 6.0963e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1795 - val_loss: 6.1046e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.1784 - val_loss: 5.9815e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.1701 - val_loss: 5.9792e-04\n",
            "Training for SNR= 4.0  sigma= 0.6309573444801932\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 111us/sample - loss: 0.1227 - val_loss: 6.1797e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1242 - val_loss: 6.2254e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.1190 - val_loss: 6.0972e-04\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1243 - val_loss: 6.0793e-04\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1206 - val_loss: 5.9751e-04\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1204 - val_loss: 5.8654e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.1188 - val_loss: 5.8903e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1201 - val_loss: 5.8712e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1229 - val_loss: 5.6534e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1215 - val_loss: 5.6738e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.1220 - val_loss: 5.6865e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1130 - val_loss: 5.5827e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1145 - val_loss: 5.5148e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1193 - val_loss: 5.5360e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1156 - val_loss: 5.3414e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1233 - val_loss: 5.2638e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1176 - val_loss: 5.3977e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1214 - val_loss: 5.3362e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.1204 - val_loss: 5.2094e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1179 - val_loss: 5.1852e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1137 - val_loss: 5.0775e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.1153 - val_loss: 5.0395e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1105 - val_loss: 5.0068e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1127 - val_loss: 4.9688e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.1158 - val_loss: 4.9208e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1140 - val_loss: 4.9452e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.1124 - val_loss: 4.8891e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1187 - val_loss: 4.8703e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1145 - val_loss: 4.8601e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1233 - val_loss: 4.8813e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1217 - val_loss: 4.7666e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1164 - val_loss: 4.7807e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.1155 - val_loss: 4.6390e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1122 - val_loss: 4.5950e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1101 - val_loss: 4.6103e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1176 - val_loss: 4.6258e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1168 - val_loss: 4.5854e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1157 - val_loss: 4.6010e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1200 - val_loss: 4.5441e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.1172 - val_loss: 4.6483e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.1213 - val_loss: 4.6015e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1200 - val_loss: 4.5660e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1163 - val_loss: 4.5541e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1166 - val_loss: 4.5283e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1230 - val_loss: 4.6045e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.1232 - val_loss: 4.5618e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.1104 - val_loss: 4.4679e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.1164 - val_loss: 4.4869e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.1167 - val_loss: 4.4336e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.1091 - val_loss: 4.4449e-04\n",
            "Training for SNR= 4.5  sigma= 0.5956621435290105\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 113us/sample - loss: 0.0739 - val_loss: 4.5123e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0816 - val_loss: 4.5863e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0789 - val_loss: 4.5578e-04\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0808 - val_loss: 4.4798e-04\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0797 - val_loss: 4.4070e-04\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0743 - val_loss: 4.2670e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0767 - val_loss: 4.2926e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0758 - val_loss: 4.2292e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0768 - val_loss: 4.2041e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0845 - val_loss: 4.2466e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0766 - val_loss: 4.2184e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0716 - val_loss: 4.1508e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0808 - val_loss: 4.2159e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0786 - val_loss: 4.1955e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0789 - val_loss: 4.1221e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0773 - val_loss: 4.1350e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0822 - val_loss: 4.1962e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0779 - val_loss: 4.1381e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0860 - val_loss: 4.1059e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0802 - val_loss: 3.9893e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0754 - val_loss: 3.9350e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0764 - val_loss: 3.9103e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0705 - val_loss: 3.9072e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0742 - val_loss: 3.8853e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0752 - val_loss: 3.8386e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0787 - val_loss: 3.9128e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0804 - val_loss: 3.8803e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0735 - val_loss: 3.8346e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0740 - val_loss: 3.8617e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0754 - val_loss: 3.8436e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0753 - val_loss: 3.7963e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.0714 - val_loss: 3.7137e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0759 - val_loss: 3.6878e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0746 - val_loss: 3.6804e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0798 - val_loss: 3.6963e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0788 - val_loss: 3.6603e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0760 - val_loss: 3.6141e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0723 - val_loss: 3.6663e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0763 - val_loss: 3.7093e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0769 - val_loss: 3.7308e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0750 - val_loss: 3.5834e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0720 - val_loss: 3.5730e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0770 - val_loss: 3.6152e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0700 - val_loss: 3.5566e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0690 - val_loss: 3.4797e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0747 - val_loss: 3.4759e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0764 - val_loss: 3.5140e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0802 - val_loss: 3.4967e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0754 - val_loss: 3.4915e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 0s 48us/sample - loss: 0.0715 - val_loss: 3.4378e-04\n",
            "Training for SNR= 5.0  sigma= 0.5623413251903491\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 118us/sample - loss: 0.0442 - val_loss: 3.4625e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0478 - val_loss: 3.5217e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0540 - val_loss: 3.5267e-04\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0484 - val_loss: 3.5388e-04\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0492 - val_loss: 3.5273e-04\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0485 - val_loss: 3.5907e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0506 - val_loss: 3.5226e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0491 - val_loss: 3.4701e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0560 - val_loss: 3.5531e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0552 - val_loss: 3.4539e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0517 - val_loss: 3.3991e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0490 - val_loss: 3.4267e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0493 - val_loss: 3.3921e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0542 - val_loss: 3.3689e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0525 - val_loss: 3.3155e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0517 - val_loss: 3.3106e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0471 - val_loss: 3.2433e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0471 - val_loss: 3.2154e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0473 - val_loss: 3.1968e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0474 - val_loss: 3.1654e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0506 - val_loss: 3.1571e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0487 - val_loss: 3.1757e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0457 - val_loss: 3.1965e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0490 - val_loss: 3.1299e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0481 - val_loss: 3.1304e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0484 - val_loss: 3.0993e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0464 - val_loss: 3.0586e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0498 - val_loss: 3.0542e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0506 - val_loss: 3.0559e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0474 - val_loss: 3.0558e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0501 - val_loss: 3.0195e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0534 - val_loss: 3.0140e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0509 - val_loss: 3.0569e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0476 - val_loss: 3.0408e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0512 - val_loss: 2.9959e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0480 - val_loss: 3.0423e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0502 - val_loss: 3.0154e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0490 - val_loss: 2.9831e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0455 - val_loss: 2.9797e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0488 - val_loss: 2.9423e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0504 - val_loss: 2.9557e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0480 - val_loss: 2.9655e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0511 - val_loss: 2.9444e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0431 - val_loss: 2.9514e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0462 - val_loss: 2.9202e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0527 - val_loss: 2.9324e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0488 - val_loss: 2.9237e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0483 - val_loss: 2.8690e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0483 - val_loss: 2.9030e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0481 - val_loss: 2.9050e-04\n",
            "Training for SNR= 5.5  sigma= 0.5308844442309884\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 118us/sample - loss: 0.0296 - val_loss: 2.8852e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 124us/sample - loss: 0.0344 - val_loss: 2.9979e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 69us/sample - loss: 0.0349 - val_loss: 3.0009e-04\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0360 - val_loss: 2.9914e-04\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0369 - val_loss: 2.9473e-04\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0384 - val_loss: 2.9043e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0340 - val_loss: 2.9711e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0327 - val_loss: 2.8869e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0304 - val_loss: 2.8765e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0339 - val_loss: 2.8255e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0332 - val_loss: 2.8277e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0342 - val_loss: 2.8168e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0325 - val_loss: 2.7946e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0353 - val_loss: 2.7741e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0320 - val_loss: 2.8374e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0341 - val_loss: 2.8250e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0324 - val_loss: 2.7745e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0315 - val_loss: 2.7905e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0341 - val_loss: 2.7630e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0316 - val_loss: 2.6933e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0362 - val_loss: 2.7041e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0346 - val_loss: 2.6747e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0318 - val_loss: 2.6973e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0340 - val_loss: 2.7232e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0318 - val_loss: 2.6892e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0346 - val_loss: 2.5921e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0342 - val_loss: 2.6101e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0339 - val_loss: 2.5583e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0304 - val_loss: 2.5662e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0375 - val_loss: 2.5994e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0348 - val_loss: 2.6837e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0353 - val_loss: 2.6562e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0353 - val_loss: 2.6012e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0333 - val_loss: 2.5888e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0310 - val_loss: 2.5669e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0337 - val_loss: 2.5497e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0328 - val_loss: 2.5399e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0294 - val_loss: 2.5172e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0320 - val_loss: 2.4719e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0324 - val_loss: 2.4810e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0302 - val_loss: 2.4800e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0317 - val_loss: 2.4955e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0306 - val_loss: 2.4593e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0315 - val_loss: 2.4436e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0321 - val_loss: 2.4481e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0315 - val_loss: 2.4754e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0320 - val_loss: 2.4662e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0322 - val_loss: 2.4538e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0324 - val_loss: 2.4771e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0326 - val_loss: 2.5109e-04\n",
            "Training for SNR= 6.0  sigma= 0.5011872336272722\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 122us/sample - loss: 0.0194 - val_loss: 2.4181e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0226 - val_loss: 2.4945e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0216 - val_loss: 2.5169e-04\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0210 - val_loss: 2.5177e-04\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0237 - val_loss: 2.5566e-04\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0282 - val_loss: 2.5840e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0240 - val_loss: 2.5679e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0231 - val_loss: 2.5480e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0240 - val_loss: 2.5653e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0244 - val_loss: 2.5742e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0263 - val_loss: 2.4966e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0263 - val_loss: 2.4902e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0240 - val_loss: 2.4892e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0226 - val_loss: 2.4746e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0235 - val_loss: 2.4479e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0244 - val_loss: 2.4455e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0239 - val_loss: 2.5122e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0251 - val_loss: 2.5572e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0251 - val_loss: 2.5482e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0257 - val_loss: 2.5087e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0251 - val_loss: 2.4503e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0262 - val_loss: 2.4422e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0277 - val_loss: 2.4918e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0283 - val_loss: 2.4629e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0259 - val_loss: 2.4859e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0271 - val_loss: 2.4588e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0230 - val_loss: 2.4903e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0272 - val_loss: 2.4511e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0228 - val_loss: 2.3978e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0237 - val_loss: 2.4041e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0226 - val_loss: 2.3866e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0232 - val_loss: 2.3915e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0282 - val_loss: 2.3642e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0211 - val_loss: 2.3522e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0200 - val_loss: 2.3004e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0209 - val_loss: 2.2860e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0210 - val_loss: 2.2567e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0232 - val_loss: 2.2541e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0223 - val_loss: 2.2155e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0227 - val_loss: 2.2084e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0210 - val_loss: 2.2184e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0229 - val_loss: 2.2121e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0229 - val_loss: 2.1732e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0212 - val_loss: 2.1864e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0223 - val_loss: 2.1929e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0221 - val_loss: 2.2291e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0204 - val_loss: 2.2240e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0229 - val_loss: 2.2061e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0232 - val_loss: 2.2038e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0232 - val_loss: 2.2339e-04\n",
            "Training for SNR= 6.5  sigma= 0.47315125896148047\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 123us/sample - loss: 0.0129 - val_loss: 2.1514e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0148 - val_loss: 2.1395e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0187 - val_loss: 2.2820e-04\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0200 - val_loss: 2.2762e-04\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0186 - val_loss: 2.3723e-04\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0187 - val_loss: 2.4225e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0199 - val_loss: 2.3830e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0206 - val_loss: 2.4217e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0176 - val_loss: 2.4055e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0227 - val_loss: 2.4042e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0193 - val_loss: 2.4359e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0184 - val_loss: 2.4112e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0210 - val_loss: 2.4987e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0206 - val_loss: 2.4310e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0178 - val_loss: 2.3177e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0205 - val_loss: 2.3168e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0197 - val_loss: 2.3644e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0180 - val_loss: 2.2908e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0209 - val_loss: 2.2786e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0172 - val_loss: 2.2447e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0183 - val_loss: 2.1927e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0178 - val_loss: 2.1973e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0167 - val_loss: 2.1997e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0157 - val_loss: 2.1895e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0142 - val_loss: 2.1938e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0160 - val_loss: 2.2045e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0158 - val_loss: 2.1888e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0143 - val_loss: 2.1587e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0170 - val_loss: 2.1527e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0157 - val_loss: 2.1546e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0138 - val_loss: 2.1354e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0195 - val_loss: 2.1345e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0152 - val_loss: 2.1348e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0186 - val_loss: 2.0996e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0158 - val_loss: 2.1179e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0175 - val_loss: 2.2262e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0167 - val_loss: 2.1415e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0168 - val_loss: 2.1344e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0166 - val_loss: 2.1189e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0152 - val_loss: 2.1224e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0178 - val_loss: 2.0802e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0155 - val_loss: 2.0940e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0165 - val_loss: 2.0859e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0171 - val_loss: 2.1005e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0154 - val_loss: 2.0459e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0185 - val_loss: 2.0127e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0165 - val_loss: 2.0112e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0143 - val_loss: 2.0126e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0157 - val_loss: 2.0473e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0134 - val_loss: 2.0224e-04\n",
            "Training for SNR= 7.0  sigma= 0.44668359215096315\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 122us/sample - loss: 0.0087 - val_loss: 1.9343e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0108 - val_loss: 2.0234e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0166 - val_loss: 2.1027e-04\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0184 - val_loss: 2.2312e-04\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0213 - val_loss: 2.4772e-04\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0219 - val_loss: 2.3715e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0189 - val_loss: 2.4351e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0203 - val_loss: 2.3718e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0184 - val_loss: 2.3245e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0190 - val_loss: 2.3718e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0199 - val_loss: 2.3465e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0177 - val_loss: 2.2804e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0171 - val_loss: 2.3375e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0156 - val_loss: 2.2957e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0151 - val_loss: 2.2631e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0148 - val_loss: 2.2729e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0157 - val_loss: 2.1591e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0150 - val_loss: 2.0986e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0147 - val_loss: 2.1466e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0173 - val_loss: 2.1688e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0159 - val_loss: 2.1437e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0140 - val_loss: 2.1405e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0140 - val_loss: 2.1058e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0149 - val_loss: 2.1753e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0126 - val_loss: 2.1095e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0106 - val_loss: 2.0630e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0119 - val_loss: 2.0273e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0107 - val_loss: 1.9881e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0124 - val_loss: 2.0038e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0138 - val_loss: 2.0196e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0101 - val_loss: 2.0312e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0144 - val_loss: 2.0597e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0131 - val_loss: 2.1874e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0107 - val_loss: 2.0174e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0142 - val_loss: 1.9775e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0132 - val_loss: 2.0634e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0114 - val_loss: 2.0333e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0109 - val_loss: 1.9855e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0145 - val_loss: 1.9712e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0119 - val_loss: 2.0229e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0094 - val_loss: 1.9674e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0126 - val_loss: 1.9387e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0123 - val_loss: 1.9145e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0109 - val_loss: 1.8839e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0126 - val_loss: 1.9446e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0105 - val_loss: 1.9410e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0112 - val_loss: 1.9182e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0118 - val_loss: 1.8787e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0133 - val_loss: 1.9039e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0111 - val_loss: 1.9113e-04\n",
            "Training for SNR= 7.5  sigma= 0.4216965034285822\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 126us/sample - loss: 0.0069 - val_loss: 1.7871e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0133 - val_loss: 1.9845e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0137 - val_loss: 2.1363e-04\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0173 - val_loss: 2.3571e-04\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0226 - val_loss: 2.8455e-04\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0249 - val_loss: 2.8584e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0209 - val_loss: 2.7189e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0226 - val_loss: 2.6161e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0213 - val_loss: 2.4309e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0160 - val_loss: 2.4973e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0203 - val_loss: 2.4463e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0172 - val_loss: 2.4773e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0180 - val_loss: 2.4166e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0143 - val_loss: 2.2435e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0124 - val_loss: 2.1832e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0136 - val_loss: 2.2620e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0140 - val_loss: 2.2468e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0119 - val_loss: 2.2577e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0128 - val_loss: 2.1485e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0110 - val_loss: 2.0489e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0120 - val_loss: 2.0540e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0120 - val_loss: 2.0727e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0126 - val_loss: 2.0548e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0102 - val_loss: 2.0873e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0099 - val_loss: 2.0649e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0111 - val_loss: 2.0569e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0101 - val_loss: 2.0456e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0083 - val_loss: 1.9845e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0105 - val_loss: 2.0102e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0091 - val_loss: 2.0741e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0097 - val_loss: 1.9915e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0085 - val_loss: 1.9948e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0110 - val_loss: 1.9991e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0084 - val_loss: 1.9534e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0093 - val_loss: 1.9319e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0094 - val_loss: 1.9425e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0082 - val_loss: 1.9593e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0081 - val_loss: 1.9358e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0085 - val_loss: 2.0252e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0073 - val_loss: 1.9621e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0094 - val_loss: 1.9388e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0077 - val_loss: 1.9219e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0072 - val_loss: 1.8868e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0079 - val_loss: 1.8963e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0071 - val_loss: 1.8497e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0068 - val_loss: 1.8045e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0085 - val_loss: 1.8289e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0083 - val_loss: 1.8537e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0087 - val_loss: 1.8884e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0092 - val_loss: 1.8589e-04\n",
            "Training for SNR= 8.0  sigma= 0.3981071705534972\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 130us/sample - loss: 0.0060 - val_loss: 1.8176e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0108 - val_loss: 2.0368e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0237 - val_loss: 3.0234e-04\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0331 - val_loss: 3.7705e-04\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0396 - val_loss: 5.1283e-04\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0313 - val_loss: 3.6526e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0230 - val_loss: 3.0570e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0193 - val_loss: 2.7675e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0161 - val_loss: 2.5919e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0142 - val_loss: 2.4187e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0105 - val_loss: 2.2208e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0092 - val_loss: 2.2349e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0092 - val_loss: 2.1105e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0086 - val_loss: 1.9817e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0073 - val_loss: 1.9551e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0069 - val_loss: 1.9533e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0073 - val_loss: 1.9889e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0058 - val_loss: 1.9560e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0093 - val_loss: 2.0298e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0076 - val_loss: 2.2714e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0078 - val_loss: 1.9668e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0076 - val_loss: 2.0750e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0070 - val_loss: 2.0035e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0084 - val_loss: 1.9296e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0071 - val_loss: 2.2887e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0067 - val_loss: 1.8810e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0074 - val_loss: 1.8599e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0058 - val_loss: 1.8733e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0071 - val_loss: 1.8504e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0058 - val_loss: 1.8711e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0066 - val_loss: 1.9943e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0086 - val_loss: 1.8899e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0083 - val_loss: 1.9423e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0072 - val_loss: 2.1193e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0095 - val_loss: 1.9387e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0075 - val_loss: 1.9134e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0082 - val_loss: 1.9205e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0093 - val_loss: 1.9854e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0099 - val_loss: 2.0000e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0078 - val_loss: 1.9027e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0099 - val_loss: 1.9352e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0077 - val_loss: 1.9530e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0087 - val_loss: 1.8992e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0093 - val_loss: 1.9024e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0074 - val_loss: 1.8773e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0089 - val_loss: 1.9242e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0091 - val_loss: 1.9162e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0081 - val_loss: 1.8971e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0074 - val_loss: 1.8346e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0064 - val_loss: 1.7811e-04\n",
            "Training for SNR= 8.5  sigma= 0.3758374042884442\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 127us/sample - loss: 0.0055 - val_loss: 1.8098e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0151 - val_loss: 2.5750e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0371 - val_loss: 0.0014\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0489 - val_loss: 0.0025\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0433 - val_loss: 0.0018\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0262 - val_loss: 3.8357e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0166 - val_loss: 3.1264e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0128 - val_loss: 2.6121e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0116 - val_loss: 2.3877e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0075 - val_loss: 2.4740e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0073 - val_loss: 2.0707e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0062 - val_loss: 2.0083e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0053 - val_loss: 2.1797e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0054 - val_loss: 1.9740e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0053 - val_loss: 1.9301e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0051 - val_loss: 1.8779e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0041 - val_loss: 1.9554e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0038 - val_loss: 1.8513e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0045 - val_loss: 1.8115e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0046 - val_loss: 1.8788e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0040 - val_loss: 1.8983e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0049 - val_loss: 1.8318e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0044 - val_loss: 1.7876e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0039 - val_loss: 1.7560e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0036 - val_loss: 1.7416e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0049 - val_loss: 2.4007e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 0s 49us/sample - loss: 0.0041 - val_loss: 1.9041e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0044 - val_loss: 1.8636e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0043 - val_loss: 1.8452e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0039 - val_loss: 1.8084e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0044 - val_loss: 1.7777e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0056 - val_loss: 1.8266e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0052 - val_loss: 1.7667e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0059 - val_loss: 1.8424e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0051 - val_loss: 1.7955e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0068 - val_loss: 2.0037e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0061 - val_loss: 1.9631e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 1s 50us/sample - loss: 0.0057 - val_loss: 1.8805e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0075 - val_loss: 1.8871e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0061 - val_loss: 1.8512e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0063 - val_loss: 2.1429e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0058 - val_loss: 1.8362e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0065 - val_loss: 1.9012e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 51us/sample - loss: 0.0062 - val_loss: 1.8466e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0063 - val_loss: 1.8587e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 0s 50us/sample - loss: 0.0057 - val_loss: 2.0464e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0055 - val_loss: 1.9453e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0053 - val_loss: 1.9623e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0062 - val_loss: 1.9182e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0061 - val_loss: 2.3031e-04\n",
            "Training for SNR= 9.0  sigma= 0.35481338923357547\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 138us/sample - loss: 0.0063 - val_loss: 2.0057e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0139 - val_loss: 2.9535e-04\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0350 - val_loss: 5.8739e-04\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0629 - val_loss: 0.0011\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0537 - val_loss: 0.0011\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0292 - val_loss: 5.9974e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0199 - val_loss: 3.5588e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0141 - val_loss: 2.8122e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0094 - val_loss: 2.5681e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0074 - val_loss: 2.1698e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0047 - val_loss: 2.0647e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0043 - val_loss: 1.9907e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0038 - val_loss: 1.8805e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0037 - val_loss: 1.8824e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0035 - val_loss: 1.7929e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0027 - val_loss: 1.7650e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0024 - val_loss: 1.8680e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0021 - val_loss: 1.7649e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0024 - val_loss: 1.6781e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0028 - val_loss: 1.6924e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0027 - val_loss: 1.6819e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0019 - val_loss: 1.6807e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0017 - val_loss: 1.6275e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0020 - val_loss: 2.0688e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0030 - val_loss: 1.6785e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0028 - val_loss: 1.6921e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0026 - val_loss: 1.7031e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0018 - val_loss: 1.6845e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0019 - val_loss: 1.6647e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0025 - val_loss: 1.6185e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0023 - val_loss: 1.6642e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0024 - val_loss: 1.6044e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0020 - val_loss: 1.7197e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 0.0019 - val_loss: 1.6096e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0016 - val_loss: 1.5725e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0023 - val_loss: 1.6454e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0022 - val_loss: 1.7688e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0025 - val_loss: 1.5821e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0022 - val_loss: 1.6652e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0018 - val_loss: 1.6507e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0024 - val_loss: 1.6639e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0022 - val_loss: 1.6017e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0030 - val_loss: 1.9260e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0031 - val_loss: 1.8267e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0032 - val_loss: 2.4174e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 0.0028 - val_loss: 1.6791e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0032 - val_loss: 1.7277e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0030 - val_loss: 1.7524e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0030 - val_loss: 1.7674e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0024 - val_loss: 1.6773e-04\n",
            "Training for SNR= 9.5  sigma= 0.33496543915782767\n",
            "Train on 10000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "10000/10000 [==============================] - 1s 134us/sample - loss: 0.0059 - val_loss: 2.6717e-04\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0407 - val_loss: 0.0037\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0931 - val_loss: 0.0043\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0723 - val_loss: 9.6225e-04\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0290 - val_loss: 5.0071e-04\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0154 - val_loss: 3.3001e-04\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0079 - val_loss: 2.7545e-04\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0073 - val_loss: 2.5802e-04\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0044 - val_loss: 2.4389e-04\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0037 - val_loss: 2.1224e-04\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0031 - val_loss: 1.9510e-04\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.0026 - val_loss: 1.8668e-04\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0023 - val_loss: 1.8251e-04\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0021 - val_loss: 1.7816e-04\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0029 - val_loss: 1.7674e-04\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0020 - val_loss: 1.7338e-04\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0018 - val_loss: 1.7107e-04\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0017 - val_loss: 1.7607e-04\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0017 - val_loss: 1.6698e-04\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0018 - val_loss: 1.6341e-04\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0018 - val_loss: 1.6287e-04\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0014 - val_loss: 1.6763e-04\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0013 - val_loss: 1.6338e-04\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0013 - val_loss: 1.6831e-04\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0011 - val_loss: 1.5677e-04\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0010 - val_loss: 1.5591e-04\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0011 - val_loss: 1.5542e-04\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0013 - val_loss: 1.6143e-04\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0011 - val_loss: 1.5899e-04\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0012 - val_loss: 1.5499e-04\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0010 - val_loss: 1.5372e-04\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 9.8403e-04 - val_loss: 1.5126e-04\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0012 - val_loss: 1.5336e-04\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 9.4524e-04 - val_loss: 1.4892e-04\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0012 - val_loss: 1.7504e-04\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 9.8168e-04 - val_loss: 1.7752e-04\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0011 - val_loss: 1.6609e-04\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0014 - val_loss: 1.7047e-04\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0011 - val_loss: 5.0551e-04\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0014 - val_loss: 1.6683e-04\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 0.0013 - val_loss: 1.7032e-04\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0014 - val_loss: 1.6119e-04\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0011 - val_loss: 1.5677e-04\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 9.3478e-04 - val_loss: 1.5963e-04\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0011 - val_loss: 1.6569e-04\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0013 - val_loss: 1.5650e-04\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0012 - val_loss: 1.5786e-04\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 9.7301e-04 - val_loss: 1.6321e-04\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0013 - val_loss: 1.6129e-04\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0012 - val_loss: 1.9225e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjVpOnoOuF0o",
        "outputId": "a40173a7-3d0b-4140-c59b-9c5cbac6412f"
      },
      "source": [
        "# Here I am using trained model\n",
        "output_display_counter = NUM_OF_INPUT_MESSAGE/4\n",
        "ber_per_iter_dl_tensor  = numpy.array(())\n",
        "times_per_iter_dl_tensor = numpy.array(())\n",
        "\n",
        "#awgn_channel_tx = GaussianNoise(0.5,input_shape=(CHANEL_SIZE,))\n",
        "\n",
        "#awgn_channel_input = tf.compat.v1.placeholder(tf.float64, [CHANEL_SIZE])\n",
        "#awgn_noise_std_dev = tf.placeholder(tf.float64)\n",
        "#awgn_noise = tf.random.normal(tf.shape(awgn_channel_input), stddev=awgn_noise_std_dev, dtype=tf.dtypes.float64)\n",
        "#awgn_channel_output = tf.add(awgn_channel_input, awgn_noise)\n",
        "\n",
        "train_init = tf.global_variables_initializer ()\n",
        "train_sess = tf.Session ()\n",
        "\n",
        "for snr in numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE):\n",
        "  total_bit_error = 0\n",
        "  total_msg_error = 0\n",
        "  total_time = 0\n",
        "  current_time = time.time()\n",
        "  sigma = Snr2Sigma (snr)\n",
        "  #awgn_channel = GaussianNoise(sigma,input_shape=(CHANEL_SIZE,))\n",
        "  #autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "  #opt = keras.optimizers.Adam(learning_rate=0.003)\n",
        "  #autoencoder.compile(optimizer=opt, loss='binary_crossentropy')\n",
        "  for i in range (NUM_OF_INPUT_MESSAGE):\n",
        "    input_message_xx = training_input_message_one_hot [i:i+1]\n",
        "    #print (\"input\", input_message_xx)\n",
        "    encoded_message = encoder.predict(input_message_xx)\n",
        "    #encoded_message = numpy.around(encoded_message > 0.5).astype(int)\n",
        "    #print(\"encoded:\",encoded_message)\n",
        "    #print (\"encoded\", encoded_message)\n",
        "    #noised_message = awgn_channel.predict (encoded_message)\n",
        "    #noised_message = commpy.channels.awgn(encoded_message, snr)\n",
        "    #noised_message = train_sess.run ([awgn_channel_output], feed_dict={awgn_noise_std_dev:sigma, awgn_channel_input:encoded_message[0]})[0].reshape([1,CHANEL_SIZE])\n",
        "    noised_message = encoded_message[0] + numpy.random.normal(0, sigma, [1,2*channel_size])\n",
        "    #print (noised_message)\n",
        "    #awgn_channel = GaussianNoise(sigma,input_shape=(CHANEL_SIZE,))\n",
        "    #noised_message = awgn_channel.predict(encoded_message)\n",
        "    #noised_message = awgn_layer (encoded_message)    \n",
        "    #print(noised_message)\n",
        "    decoded_message = decoder.predict(noised_message)\n",
        "    #print (\"decoded1:\", decoded_message)\n",
        "    #decoded_message = train_sess.run ([decoder_output], feed_dict={decoder_input_x:decoded_message})\n",
        "    #print (\"decoded2:\", decoded_message)\n",
        "    #decoded_message = autoencoder.predict(input_message_xx)\n",
        "    #decoded_message = numpy.around(decoded_message[0]).astype(int)\n",
        "    #print (\"decoded3:\", decoded_message)\n",
        "    #decoded_message = numpy.around(decoded_message > 0.5).astype(int)\n",
        "    #print (\"decoded:\", decoded_message)\n",
        "    #print (\".\")\n",
        "    #autoencoder = Model(input_message_x, decoder(awgn_channel(encoder(input_message_x))))\n",
        "    #decoded_message = autoencoder.predict(input_message_xx)\n",
        "    #print (\"output1\", numpy.argmax(training_input_message_one_hot[i]))\n",
        "    #print (\"output2\", numpy.argmax(decoded_message[0]))\n",
        "    #print (\"output2\", training_input_message[0][i])\n",
        "    if (numpy.argmax(training_input_message_one_hot[i]) != numpy.argmax(decoded_message[0])):\n",
        "      total_msg_error = total_msg_error + 1\n",
        "      #print (\"Error\")\n",
        "    if (i+1) % output_display_counter == 0:\n",
        "      total_time = timer_update(i, current_time,total_time, output_display_counter)\n",
        "  ber = float(total_msg_error)/NUM_OF_INPUT_MESSAGE\n",
        "  print('SNR: {:04.3f}:\\n -> BER: {:03.2f}\\n -> Total Time: {:03.2f}s'.format(snr,ber,total_time))\n",
        "  ber_per_iter_dl_tensor=numpy.append(ber_per_iter_dl_tensor ,ber)\n",
        "  times_per_iter_dl_tensor=numpy.append(times_per_iter_dl_tensor, total_time)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR: 0.000 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 0.000 - Iter: 500 - Last 250.0 iterations took 1.52s\n",
            "SNR: 0.000 - Iter: 750 - Last 250.0 iterations took 2.29s\n",
            "SNR: 0.000 - Iter: 1000 - Last 250.0 iterations took 3.05s\n",
            "SNR: 0.000:\n",
            " -> BER: 0.51\n",
            " -> Total Time: 7.63s\n",
            "SNR: 0.500 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 0.500 - Iter: 500 - Last 250.0 iterations took 1.52s\n",
            "SNR: 0.500 - Iter: 750 - Last 250.0 iterations took 2.31s\n",
            "SNR: 0.500 - Iter: 1000 - Last 250.0 iterations took 3.09s\n",
            "SNR: 0.500:\n",
            " -> BER: 0.44\n",
            " -> Total Time: 7.67s\n",
            "SNR: 1.000 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 1.000 - Iter: 500 - Last 250.0 iterations took 1.52s\n",
            "SNR: 1.000 - Iter: 750 - Last 250.0 iterations took 2.30s\n",
            "SNR: 1.000 - Iter: 1000 - Last 250.0 iterations took 3.09s\n",
            "SNR: 1.000:\n",
            " -> BER: 0.35\n",
            " -> Total Time: 7.68s\n",
            "SNR: 1.500 - Iter: 250 - Last 250.0 iterations took 0.77s\n",
            "SNR: 1.500 - Iter: 500 - Last 250.0 iterations took 1.53s\n",
            "SNR: 1.500 - Iter: 750 - Last 250.0 iterations took 2.30s\n",
            "SNR: 1.500 - Iter: 1000 - Last 250.0 iterations took 3.06s\n",
            "SNR: 1.500:\n",
            " -> BER: 0.30\n",
            " -> Total Time: 7.66s\n",
            "SNR: 2.000 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 2.000 - Iter: 500 - Last 250.0 iterations took 1.52s\n",
            "SNR: 2.000 - Iter: 750 - Last 250.0 iterations took 2.29s\n",
            "SNR: 2.000 - Iter: 1000 - Last 250.0 iterations took 3.06s\n",
            "SNR: 2.000:\n",
            " -> BER: 0.26\n",
            " -> Total Time: 7.64s\n",
            "SNR: 2.500 - Iter: 250 - Last 250.0 iterations took 0.78s\n",
            "SNR: 2.500 - Iter: 500 - Last 250.0 iterations took 1.55s\n",
            "SNR: 2.500 - Iter: 750 - Last 250.0 iterations took 2.32s\n",
            "SNR: 2.500 - Iter: 1000 - Last 250.0 iterations took 3.08s\n",
            "SNR: 2.500:\n",
            " -> BER: 0.18\n",
            " -> Total Time: 7.73s\n",
            "SNR: 3.000 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 3.000 - Iter: 500 - Last 250.0 iterations took 1.53s\n",
            "SNR: 3.000 - Iter: 750 - Last 250.0 iterations took 2.30s\n",
            "SNR: 3.000 - Iter: 1000 - Last 250.0 iterations took 3.05s\n",
            "SNR: 3.000:\n",
            " -> BER: 0.15\n",
            " -> Total Time: 7.63s\n",
            "SNR: 3.500 - Iter: 250 - Last 250.0 iterations took 0.78s\n",
            "SNR: 3.500 - Iter: 500 - Last 250.0 iterations took 1.53s\n",
            "SNR: 3.500 - Iter: 750 - Last 250.0 iterations took 2.29s\n",
            "SNR: 3.500 - Iter: 1000 - Last 250.0 iterations took 3.06s\n",
            "SNR: 3.500:\n",
            " -> BER: 0.13\n",
            " -> Total Time: 7.66s\n",
            "SNR: 4.000 - Iter: 250 - Last 250.0 iterations took 0.78s\n",
            "SNR: 4.000 - Iter: 500 - Last 250.0 iterations took 1.54s\n",
            "SNR: 4.000 - Iter: 750 - Last 250.0 iterations took 2.32s\n",
            "SNR: 4.000 - Iter: 1000 - Last 250.0 iterations took 3.08s\n",
            "SNR: 4.000:\n",
            " -> BER: 0.07\n",
            " -> Total Time: 7.73s\n",
            "SNR: 4.500 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 4.500 - Iter: 500 - Last 250.0 iterations took 1.56s\n",
            "SNR: 4.500 - Iter: 750 - Last 250.0 iterations took 2.35s\n",
            "SNR: 4.500 - Iter: 1000 - Last 250.0 iterations took 3.12s\n",
            "SNR: 4.500:\n",
            " -> BER: 0.06\n",
            " -> Total Time: 7.79s\n",
            "SNR: 5.000 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 5.000 - Iter: 500 - Last 250.0 iterations took 1.53s\n",
            "SNR: 5.000 - Iter: 750 - Last 250.0 iterations took 2.29s\n",
            "SNR: 5.000 - Iter: 1000 - Last 250.0 iterations took 3.05s\n",
            "SNR: 5.000:\n",
            " -> BER: 0.04\n",
            " -> Total Time: 7.63s\n",
            "SNR: 5.500 - Iter: 250 - Last 250.0 iterations took 0.78s\n",
            "SNR: 5.500 - Iter: 500 - Last 250.0 iterations took 1.54s\n",
            "SNR: 5.500 - Iter: 750 - Last 250.0 iterations took 2.30s\n",
            "SNR: 5.500 - Iter: 1000 - Last 250.0 iterations took 3.08s\n",
            "SNR: 5.500:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 7.70s\n",
            "SNR: 6.000 - Iter: 250 - Last 250.0 iterations took 0.75s\n",
            "SNR: 6.000 - Iter: 500 - Last 250.0 iterations took 1.52s\n",
            "SNR: 6.000 - Iter: 750 - Last 250.0 iterations took 2.29s\n",
            "SNR: 6.000 - Iter: 1000 - Last 250.0 iterations took 3.05s\n",
            "SNR: 6.000:\n",
            " -> BER: 0.02\n",
            " -> Total Time: 7.61s\n",
            "SNR: 6.500 - Iter: 250 - Last 250.0 iterations took 0.77s\n",
            "SNR: 6.500 - Iter: 500 - Last 250.0 iterations took 1.53s\n",
            "SNR: 6.500 - Iter: 750 - Last 250.0 iterations took 2.30s\n",
            "SNR: 6.500 - Iter: 1000 - Last 250.0 iterations took 3.08s\n",
            "SNR: 6.500:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 7.67s\n",
            "SNR: 7.000 - Iter: 250 - Last 250.0 iterations took 0.79s\n",
            "SNR: 7.000 - Iter: 500 - Last 250.0 iterations took 1.56s\n",
            "SNR: 7.000 - Iter: 750 - Last 250.0 iterations took 2.33s\n",
            "SNR: 7.000 - Iter: 1000 - Last 250.0 iterations took 3.11s\n",
            "SNR: 7.000:\n",
            " -> BER: 0.01\n",
            " -> Total Time: 7.79s\n",
            "SNR: 7.500 - Iter: 250 - Last 250.0 iterations took 0.77s\n",
            "SNR: 7.500 - Iter: 500 - Last 250.0 iterations took 1.53s\n",
            "SNR: 7.500 - Iter: 750 - Last 250.0 iterations took 2.32s\n",
            "SNR: 7.500 - Iter: 1000 - Last 250.0 iterations took 3.08s\n",
            "SNR: 7.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 7.71s\n",
            "SNR: 8.000 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 8.000 - Iter: 500 - Last 250.0 iterations took 1.53s\n",
            "SNR: 8.000 - Iter: 750 - Last 250.0 iterations took 2.28s\n",
            "SNR: 8.000 - Iter: 1000 - Last 250.0 iterations took 3.06s\n",
            "SNR: 8.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 7.62s\n",
            "SNR: 8.500 - Iter: 250 - Last 250.0 iterations took 0.79s\n",
            "SNR: 8.500 - Iter: 500 - Last 250.0 iterations took 1.56s\n",
            "SNR: 8.500 - Iter: 750 - Last 250.0 iterations took 2.32s\n",
            "SNR: 8.500 - Iter: 1000 - Last 250.0 iterations took 3.07s\n",
            "SNR: 8.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 7.73s\n",
            "SNR: 9.000 - Iter: 250 - Last 250.0 iterations took 0.76s\n",
            "SNR: 9.000 - Iter: 500 - Last 250.0 iterations took 1.52s\n",
            "SNR: 9.000 - Iter: 750 - Last 250.0 iterations took 2.30s\n",
            "SNR: 9.000 - Iter: 1000 - Last 250.0 iterations took 3.06s\n",
            "SNR: 9.000:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 7.64s\n",
            "SNR: 9.500 - Iter: 250 - Last 250.0 iterations took 0.78s\n",
            "SNR: 9.500 - Iter: 500 - Last 250.0 iterations took 1.54s\n",
            "SNR: 9.500 - Iter: 750 - Last 250.0 iterations took 2.30s\n",
            "SNR: 9.500 - Iter: 1000 - Last 250.0 iterations took 3.06s\n",
            "SNR: 9.500:\n",
            " -> BER: 0.00\n",
            " -> Total Time: 7.68s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "cALSMP2YvKvC",
        "outputId": "367d0cc9-bfb6-4dcb-8609-cebcbd38b3b4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "snrs = numpy.arange (SNR_BEGIN, SNR_END, SNR_STEP_SIZE)\n",
        "fig, (ax1) = plt.subplots(1,1,figsize=(8,6))\n",
        "ax1.semilogy(snrs,bler_per_iter_ldpc_itpp_psk_4,'', label=\"itpp-ldpc(18,9)-qpsk(channel=9)\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,ber_per_iter_dl_tensor,'', label=\"ai-dl(input=9,channel=9)\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_commpy_psk_2,'', label=\"commpy-psk2-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_commpy_psk_4,'', label=\"commpy-psk4-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_uncoded_itpp_psk_2,'', label=\"itpp-psk2-uncoded\") # plot BER vs SNR\n",
        "ax1.semilogy(snrs,bler_per_iter_ham_itpp_psk_4,'', label=\"itpp-ham(7,4)(input=8,channel=7)\") # plot BER vs SNR\n",
        "ax1.set_ylabel('BLER')\n",
        "ax1.set_title('Arch-2 ({},{},{})'.format(input_message_length,2*input_message_length, channel_size))\n",
        "#ax2.plot(snrs,times_per_iter_pyldpc,'', label=\"pyldpc\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_tensor,'', label=\"tensor\") # plot decode timing for different SNRs\n",
        "#ax2.plot(snrs,times_per_iter_awgn,'', label=\"commpy-awgn\") # plot decode timing for different SNRs\n",
        "#ax2.set_xlabel('$E_b/$N_0$')\n",
        "#ax2.set_ylabel('Decoding Time [s]')\n",
        "#ax2.annotate('Total Runtime: pyldpc:{:03.2f}s awgn:{:03.2f}s tensor:{:03.2f}s'.format(numpy.sum(times_per_iter_pyldpc), \n",
        "#            numpy.sum(times_per_iter_awgn), numpy.sum(times_per_iter_tensor)),\n",
        "#            xy=(1, 0.35), xycoords='axes fraction',\n",
        "#            xytext=(-20, 20), textcoords='offset pixels',\n",
        "#            horizontalalignment='right',\n",
        "#            verticalalignment='bottom')\n",
        "plt.savefig('ldpc_ber_{}_{}.png'.format(2*channel_size,input_message_length))\n",
        "plt.legend ()\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}